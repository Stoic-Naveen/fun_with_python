{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPN0BZgvF3ZY/Tzu7QFjO3q"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical Questions"
      ],
      "metadata": {
        "id": "xXU7lncf-Cip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.) What is a Decision Tree, and how does it work?\n",
        "\n",
        "- A Decision Tree is a flowchart-like structure where each internal node represents a \"test\" on an attribute (feature), each branch represents the outcome of the test, and each leaf node represents a class label (for classification) or a predicted value (for regression).  It works by recursively partitioning the dataset into subsets based on the attribute that best splits the data according to a certain criterion (e.g., Gini impurity or entropy), continuing until a stopping condition is met.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2.) What are impurity measures in Decision Trees?\n",
        "\n",
        "- Impurity measures are used to evaluate the homogeneity of the data within a subset.  They quantify the degree to which data points within a subset belong to the same class. Common impurity measures include Gini impurity and entropy (information gain).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3.) What is the mathematical formula for Gini Impurity?\n",
        "\n",
        "- The Gini Impurity for a dataset D is calculated as :\n",
        "\n",
        "- Gini(D) = 1 - Σ (p_i)^2\n",
        "\n",
        "- where p_i is the proportion of data points in D that belong to class i.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4.) What is the mathematical formula for Entropy?\n",
        "\n",
        "- The Entropy for a dataset D is calculated as :\n",
        "\n",
        "- Entropy(D) = - Σ p_i * log2(p_i)\n",
        "\n",
        "- where p_i is the proportion of data points in D that belong to class i.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5.) What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "- Information Gain measures the reduction in entropy achieved by splitting a dataset on a particular attribute.  It is used to determine the best attribute to split on at each node of the tree. The attribute with the highest information gain is chosen as the splitting attribute.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6.) What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "- Both Gini impurity and entropy are used to measure the impurity of a dataset.  Entropy uses the logarithm, making it computationally more expensive than Gini impurity.  Gini impurity is generally faster to compute, while entropy might provide slightly more refined splits.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "7.) What is the mathematical explanation behind Decision Trees?\n",
        "\n",
        "- Decision Trees are based on recursively partitioning the feature space.  At each node, the algorithm selects the feature and the threshold that optimizes the chosen impurity measure (e.g., minimizes Gini impurity or maximizes information gain).  This process continues until a stopping criterion is met, such as reaching a maximum depth or having too few samples in a leaf node.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "8.) What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "- Pre-pruning involves setting criteria to stop the growth of the decision tree early.  This could include limiting the maximum depth of the tree, setting a minimum number of samples required to split a node, or setting a minimum number of samples required for a leaf node.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "9.) What is Post-Pruning in Decision Trees?\n",
        "\n",
        "- Post-pruning involves growing the decision tree fully and then removing subtrees from the fully grown tree.  This is typically done by evaluating the impact of removing a subtree on a validation set and pruning subtrees that do not improve generalization performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "10.) What is the difference between Pre-Pruning and Post-Pruning?\n",
        "\n",
        "- Pre-pruning stops the tree's growth early to avoid overfitting, while post-pruning allows the tree to grow fully and then prunes it back.  Pre-pruning is generally faster but might lead to underfitting, while post-pruning is more computationally expensive but can lead to better generalization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "11.) What is a Decision Tree Regressor?\n",
        "\n",
        "- A Decision Tree Regressor is a Decision Tree used for regression tasks, where the goal is to predict a continuous output variable.  Instead of predicting class labels, leaf nodes in a regression tree contain predicted values, typically the mean or median of the target variable for the samples in that leaf.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "12.) What are the advantages and disadvantages of Decision Trees?\n",
        "\n",
        "- Advantages :\n",
        "- Easy to understand and interpret.\n",
        "- Can handle both categorical and numerical data.\n",
        "- Require little data preparation.\n",
        "- Able to handle multi-output problems.\n",
        "- Disadvantages :\n",
        "- prone to overfitting.\n",
        "- Can be unstable : small variations in the data can result in a completely different tree.\n",
        "- Can be biased if some classes dominate.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "13.) How does a Decision Tree handle missing values?\n",
        "\n",
        "- Decision Trees can handle missing values in several ways.  Some algorithms can infer the missing values by considering the values of other attributes.  scikit-learn's Decision Trees can handle missing values by directing samples with missing values to the branch that maximizes the information gain.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "14.) How does a Decision Tree handle categorical features?\n",
        "\n",
        "- Decision Trees can handle categorical features by splitting the tree based on the categories.  For nominal features, the tree can split on whether a data point belongs to a specific category or not. For ordinal features, the tree can use the inherent ordering to make splits.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "15.) What are some real-world applications of Decision Trees?\n",
        "\n",
        "- Decision Trees are used in various applications, including :\n",
        "- Credit risk assessment\n",
        "- Medical diagnosis\n",
        "- Fraud detection\n",
        "- Customer churn prediction\n",
        "- Recommender systems"
      ],
      "metadata": {
        "id": "2ehNC_Mx-G4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Questions"
      ],
      "metadata": {
        "id": "7zyxx7kLAA41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.) Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy."
      ],
      "metadata": {
        "id": "OO8-2ZuZAHyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFuTQEeTBpqp",
        "outputId": "87cb501f-dfea-4a03-ef49-60e0e6dd2528"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.) Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances."
      ],
      "metadata": {
        "id": "acyO6Bv0AHpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Feature Importances:\", clf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64g4YLvtBqIU",
        "outputId": "a3815926-a42d-4aa1-f263-0b81f7067632"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.         0.01667014 0.40593501 0.57739485]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.) Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy."
      ],
      "metadata": {
        "id": "3LMzd3y4AHgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='entropy')\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvMbahvPBqgj",
        "outputId": "10fdeb2c-079d-49ea-f6c5-2658dd3c4d4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.) Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "M5qMblMiAHW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X_train, y_train)\n",
        "y_pred = reg.predict(X_test)\n",
        "print(f\"MSE: {mean_squared_error(y_test, y_pred):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkbAJVbTBure",
        "outputId": "ee3c46bf-f77c-4a0e-86e9-4ef0e736d337"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 4626.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.) Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz."
      ],
      "metadata": {
        "id": "EycratY9AHNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz > /dev/null\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "from IPython.display import display\n",
        "\n",
        "# Load dataset and train classifier\n",
        "iris = load_iris()\n",
        "clf = DecisionTreeClassifier().fit(iris.data, iris.target)\n",
        "\n",
        "# Generate Graphviz data\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "# Display directly in notebook\n",
        "graph = graphviz.Source(dot_data)\n",
        "display(graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "RMbaR0akBudJ",
        "outputId": "f2a717ce-7c34-42de-d9b4-c82269069ab3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"852pt\" height=\"671pt\"\n viewBox=\"0.00 0.00 852.00 671.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 667)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-667 848,-667 848,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M507.5,-663C507.5,-663 385.5,-663 385.5,-663 379.5,-663 373.5,-657 373.5,-651 373.5,-651 373.5,-592 373.5,-592 373.5,-586 379.5,-580 385.5,-580 385.5,-580 507.5,-580 507.5,-580 513.5,-580 519.5,-586 519.5,-592 519.5,-592 519.5,-651 519.5,-651 519.5,-657 513.5,-663 507.5,-663\"/>\n<text text-anchor=\"start\" x=\"381.5\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 0.8</text>\n<text text-anchor=\"start\" x=\"411\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n<text text-anchor=\"start\" x=\"401.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 150</text>\n<text text-anchor=\"start\" x=\"388.5\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 50, 50]</text>\n<text text-anchor=\"start\" x=\"403\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M416,-536.5C416,-536.5 323,-536.5 323,-536.5 317,-536.5 311,-530.5 311,-524.5 311,-524.5 311,-480.5 311,-480.5 311,-474.5 317,-468.5 323,-468.5 323,-468.5 416,-468.5 416,-468.5 422,-468.5 428,-474.5 428,-480.5 428,-480.5 428,-524.5 428,-524.5 428,-530.5 422,-536.5 416,-536.5\"/>\n<text text-anchor=\"start\" x=\"341.5\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"328.5\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n<text text-anchor=\"start\" x=\"319\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 0, 0]</text>\n<text text-anchor=\"start\" x=\"326\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M419.79,-579.91C412.38,-568.65 404.33,-556.42 396.88,-545.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"399.75,-543.1 391.33,-536.67 393.9,-546.94 399.75,-543.1\"/>\n<text text-anchor=\"middle\" x=\"386.28\" y=\"-557.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M588.5,-544C588.5,-544 458.5,-544 458.5,-544 452.5,-544 446.5,-538 446.5,-532 446.5,-532 446.5,-473 446.5,-473 446.5,-467 452.5,-461 458.5,-461 458.5,-461 588.5,-461 588.5,-461 594.5,-461 600.5,-467 600.5,-473 600.5,-473 600.5,-532 600.5,-532 600.5,-538 594.5,-544 588.5,-544\"/>\n<text text-anchor=\"start\" x=\"454.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n<text text-anchor=\"start\" x=\"495.5\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"478.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n<text text-anchor=\"start\" x=\"469\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 50, 50]</text>\n<text text-anchor=\"start\" x=\"471\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M473.21,-579.91C479.01,-571.1 485.2,-561.7 491.18,-552.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"494.26,-554.3 496.83,-544.02 488.41,-550.45 494.26,-554.3\"/>\n<text text-anchor=\"middle\" x=\"501.88\" y=\"-564.81\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#4de88e\" stroke=\"black\" d=\"M480,-425C480,-425 345,-425 345,-425 339,-425 333,-419 333,-413 333,-413 333,-354 333,-354 333,-348 339,-342 345,-342 345,-342 480,-342 480,-342 486,-342 492,-348 492,-354 492,-354 492,-413 492,-413 492,-419 486,-425 480,-425\"/>\n<text text-anchor=\"start\" x=\"341\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\n<text text-anchor=\"start\" x=\"377\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.168</text>\n<text text-anchor=\"start\" x=\"371.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 54</text>\n<text text-anchor=\"start\" x=\"362\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 49, 5]</text>\n<text text-anchor=\"start\" x=\"360\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M484.99,-460.91C476.29,-451.74 466.98,-441.93 458.03,-432.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"460.36,-429.87 450.94,-425.02 455.29,-434.68 460.36,-429.87\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#843de6\" stroke=\"black\" d=\"M702,-425C702,-425 567,-425 567,-425 561,-425 555,-419 555,-413 555,-413 555,-354 555,-354 555,-348 561,-342 567,-342 567,-342 702,-342 702,-342 708,-342 714,-348 714,-354 714,-354 714,-413 714,-413 714,-419 708,-425 702,-425\"/>\n<text text-anchor=\"start\" x=\"563\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n<text text-anchor=\"start\" x=\"599\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.043</text>\n<text text-anchor=\"start\" x=\"593.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\n<text text-anchor=\"start\" x=\"584\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 45]</text>\n<text text-anchor=\"start\" x=\"586\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>2&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M562.01,-460.91C570.71,-451.74 580.02,-441.93 588.97,-432.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"591.71,-434.68 596.06,-425.02 586.64,-429.87 591.71,-434.68\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#3de684\" stroke=\"black\" d=\"M260.5,-306C260.5,-306 130.5,-306 130.5,-306 124.5,-306 118.5,-300 118.5,-294 118.5,-294 118.5,-235 118.5,-235 118.5,-229 124.5,-223 130.5,-223 130.5,-223 260.5,-223 260.5,-223 266.5,-223 272.5,-229 272.5,-235 272.5,-235 272.5,-294 272.5,-294 272.5,-300 266.5,-306 260.5,-306\"/>\n<text text-anchor=\"start\" x=\"126.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.65</text>\n<text text-anchor=\"start\" x=\"160\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.041</text>\n<text text-anchor=\"start\" x=\"154.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 48</text>\n<text text-anchor=\"start\" x=\"145\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 47, 1]</text>\n<text text-anchor=\"start\" x=\"143\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M337.21,-341.91C318.61,-331.88 298.57,-321.07 279.59,-310.84\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"281.12,-307.69 270.65,-306.02 277.8,-313.85 281.12,-307.69\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M477.5,-306C477.5,-306 347.5,-306 347.5,-306 341.5,-306 335.5,-300 335.5,-294 335.5,-294 335.5,-235 335.5,-235 335.5,-229 341.5,-223 347.5,-223 347.5,-223 477.5,-223 477.5,-223 483.5,-223 489.5,-229 489.5,-235 489.5,-235 489.5,-294 489.5,-294 489.5,-300 483.5,-306 477.5,-306\"/>\n<text text-anchor=\"start\" x=\"343.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.55</text>\n<text text-anchor=\"start\" x=\"377\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"375\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"365.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n<text text-anchor=\"start\" x=\"364\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>3&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M412.5,-341.91C412.5,-333.65 412.5,-324.86 412.5,-316.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"416,-316.02 412.5,-306.02 409,-316.02 416,-316.02\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-179.5C109,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 109,-111.5 109,-111.5 115,-111.5 121,-117.5 121,-123.5 121,-123.5 121,-167.5 121,-167.5 121,-173.5 115,-179.5 109,-179.5\"/>\n<text text-anchor=\"start\" x=\"32.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 47</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 47, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 4&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M148.66,-222.91C135.04,-211.1 120.17,-198.22 106.6,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"108.62,-183.57 98.77,-179.67 104.03,-188.86 108.62,-183.57\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-179.5C240,-179.5 151,-179.5 151,-179.5 145,-179.5 139,-173.5 139,-167.5 139,-167.5 139,-123.5 139,-123.5 139,-117.5 145,-111.5 151,-111.5 151,-111.5 240,-111.5 240,-111.5 246,-111.5 252,-117.5 252,-123.5 252,-123.5 252,-167.5 252,-167.5 252,-173.5 246,-179.5 240,-179.5\"/>\n<text text-anchor=\"start\" x=\"167.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 4&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>4&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M195.5,-222.91C195.5,-212.2 195.5,-200.62 195.5,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"199,-189.67 195.5,-179.67 192,-189.67 199,-189.67\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M371,-179.5C371,-179.5 282,-179.5 282,-179.5 276,-179.5 270,-173.5 270,-167.5 270,-167.5 270,-123.5 270,-123.5 270,-117.5 276,-111.5 282,-111.5 282,-111.5 371,-111.5 371,-111.5 377,-111.5 383,-117.5 383,-123.5 383,-123.5 383,-167.5 383,-167.5 383,-173.5 377,-179.5 371,-179.5\"/>\n<text text-anchor=\"start\" x=\"298.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"289\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"279.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n<text text-anchor=\"start\" x=\"278\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M382.66,-222.91C374.31,-211.54 365.22,-199.18 356.84,-187.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"359.62,-185.65 350.88,-179.67 353.98,-189.8 359.62,-185.65\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M548,-187C548,-187 413,-187 413,-187 407,-187 401,-181 401,-175 401,-175 401,-116 401,-116 401,-110 407,-104 413,-104 413,-104 548,-104 548,-104 554,-104 560,-110 560,-116 560,-116 560,-175 560,-175 560,-181 554,-187 548,-187\"/>\n<text text-anchor=\"start\" x=\"409\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.45</text>\n<text text-anchor=\"start\" x=\"445\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"443\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"433.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n<text text-anchor=\"start\" x=\"428\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M436.09,-222.91C441.16,-214.2 446.56,-204.9 451.79,-195.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"454.95,-197.43 456.95,-187.02 448.9,-193.91 454.95,-197.43\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M461,-68C461,-68 364,-68 364,-68 358,-68 352,-62 352,-56 352,-56 352,-12 352,-12 352,-6 358,0 364,0 364,0 461,0 461,0 467,0 473,-6 473,-12 473,-12 473,-56 473,-56 473,-62 467,-68 461,-68\"/>\n<text text-anchor=\"start\" x=\"384.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"375\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"365.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"360\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M455.18,-103.73C449.74,-94.97 443.99,-85.7 438.52,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"441.43,-74.95 433.18,-68.3 435.48,-78.64 441.43,-74.95\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M592,-68C592,-68 503,-68 503,-68 497,-68 491,-62 491,-56 491,-56 491,-12 491,-12 491,-6 497,0 503,0 503,0 592,0 592,0 598,0 604,-6 604,-12 604,-12 604,-56 604,-56 604,-62 598,-68 592,-68\"/>\n<text text-anchor=\"start\" x=\"519.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"510\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"500.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"499\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M505.45,-103.73C510.81,-94.97 516.48,-85.7 521.86,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"524.89,-78.66 527.12,-68.3 518.92,-75 524.89,-78.66\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M697,-306C697,-306 572,-306 572,-306 566,-306 560,-300 560,-294 560,-294 560,-235 560,-235 560,-229 566,-223 572,-223 572,-223 697,-223 697,-223 703,-223 709,-229 709,-235 709,-235 709,-294 709,-294 709,-300 703,-306 697,-306\"/>\n<text text-anchor=\"start\" x=\"568\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal width (cm) ≤ 3.1</text>\n<text text-anchor=\"start\" x=\"599\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"597\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"587.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n<text text-anchor=\"start\" x=\"586\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 12&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>12&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M634.5,-341.91C634.5,-333.65 634.5,-324.86 634.5,-316.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"638,-316.02 634.5,-306.02 631,-316.02 638,-316.02\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M832,-298.5C832,-298.5 739,-298.5 739,-298.5 733,-298.5 727,-292.5 727,-286.5 727,-286.5 727,-242.5 727,-242.5 727,-236.5 733,-230.5 739,-230.5 739,-230.5 832,-230.5 832,-230.5 838,-230.5 844,-236.5 844,-242.5 844,-242.5 844,-286.5 844,-286.5 844,-292.5 838,-298.5 832,-298.5\"/>\n<text text-anchor=\"start\" x=\"757.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"744.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n<text text-anchor=\"start\" x=\"735\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 43]</text>\n<text text-anchor=\"start\" x=\"737\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 12&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>12&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M686.89,-341.91C702.27,-329.99 719.07,-316.98 734.37,-305.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"736.93,-307.56 742.69,-298.67 732.65,-302.03 736.93,-307.56\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M679,-179.5C679,-179.5 590,-179.5 590,-179.5 584,-179.5 578,-173.5 578,-167.5 578,-167.5 578,-123.5 578,-123.5 578,-117.5 584,-111.5 590,-111.5 590,-111.5 679,-111.5 679,-111.5 685,-111.5 691,-117.5 691,-123.5 691,-123.5 691,-167.5 691,-167.5 691,-173.5 685,-179.5 679,-179.5\"/>\n<text text-anchor=\"start\" x=\"606.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"597\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"587.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n<text text-anchor=\"start\" x=\"586\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 13&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>13&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M634.5,-222.91C634.5,-212.2 634.5,-200.62 634.5,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"638,-189.67 634.5,-179.67 631,-189.67 638,-189.67\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M818,-179.5C818,-179.5 721,-179.5 721,-179.5 715,-179.5 709,-173.5 709,-167.5 709,-167.5 709,-123.5 709,-123.5 709,-117.5 715,-111.5 721,-111.5 721,-111.5 818,-111.5 818,-111.5 824,-111.5 830,-117.5 830,-123.5 830,-123.5 830,-167.5 830,-167.5 830,-173.5 824,-179.5 818,-179.5\"/>\n<text text-anchor=\"start\" x=\"741.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"732\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"722.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n<text text-anchor=\"start\" x=\"717\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 13&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>13&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M681.34,-222.91C694.96,-211.1 709.83,-198.22 723.4,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"725.97,-188.86 731.23,-179.67 721.38,-183.57 725.97,-188.86\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7c4e9a32fa10>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.) Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree."
      ],
      "metadata": {
        "id": "8AIDLvCXAHEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Full tree\n",
        "clf_full = DecisionTreeClassifier()\n",
        "clf_full.fit(X_train, y_train)\n",
        "print(f\"Full Tree Accuracy: {accuracy_score(y_test, clf_full.predict(X_test)):.2f}\")\n",
        "\n",
        "# Limited depth\n",
        "clf_3 = DecisionTreeClassifier(max_depth=3)\n",
        "clf_3.fit(X_train, y_train)\n",
        "print(f\"Max Depth 3 Accuracy: {accuracy_score(y_test, clf_3.predict(X_test)):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Xn1j_IRBt8m",
        "outputId": "9df5e345-8e35-418c-8ff8-2f6199f530f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Tree Accuracy: 1.00\n",
            "Max Depth 3 Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.) Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree."
      ],
      "metadata": {
        "id": "M9frtCicAG7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Default\n",
        "clf_default = DecisionTreeClassifier()\n",
        "clf_default.fit(X_train, y_train)\n",
        "print(f\"Default Accuracy: {accuracy_score(y_test, clf_default.predict(X_test)):.2f}\")\n",
        "\n",
        "# With min_samples_split\n",
        "clf_min5 = DecisionTreeClassifier(min_samples_split=5)\n",
        "clf_min5.fit(X_train, y_train)\n",
        "print(f\"Min Samples 5 Accuracy: {accuracy_score(y_test, clf_min5.predict(X_test)):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4FT2XndBtp2",
        "outputId": "d46d31fc-228d-4543-a7fe-f767d71ccacb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default Accuracy: 1.00\n",
            "Min Samples 5 Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.) Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data."
      ],
      "metadata": {
        "id": "kcGzO3xWAGxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Unscaled\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "print(f\"Unscaled Accuracy: {accuracy_score(y_test, clf.predict(X_test)):.2f}\")\n",
        "\n",
        "# Scaled\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "clf_scaled = DecisionTreeClassifier()\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "print(f\"Scaled Accuracy: {accuracy_score(y_test, clf_scaled.predict(X_test_scaled)):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaRpTJWzBtYq",
        "outputId": "50963aad-b5c8-46e1-aafc-8b4a6ced344c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unscaled Accuracy: 1.00\n",
            "Scaled Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.) Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification."
      ],
      "metadata": {
        "id": "9CpckYcTAGoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = OneVsRestClassifier(DecisionTreeClassifier())\n",
        "clf.fit(X_train, y_train)\n",
        "print(f\"OvR Accuracy: {accuracy_score(y_test, clf.predict(X_test)):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHgHCpAmBs9l",
        "outputId": "a23e59a2-4ac8-4d50-d4a1-deb8391ba04c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvR Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.) Write a Python program to train a Decision Tree Classifier and display the feature importance scores."
      ],
      "metadata": {
        "id": "ffm_lIQ6AGet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "clf = DecisionTreeClassifier().fit(iris.data, iris.target)\n",
        "print(\"Feature Importances:\", clf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYH-mBm_Bsts",
        "outputId": "6eda978b-f059-4e9b-fbbc-f2a1a95d7829"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.02666667 0.         0.05072262 0.92261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.) Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree."
      ],
      "metadata": {
        "id": "zn6C79-jAGUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Unrestricted\n",
        "reg_full = DecisionTreeRegressor()\n",
        "reg_full.fit(X_train, y_train)\n",
        "print(f\"Full Tree MSE: {mean_squared_error(y_test, reg_full.predict(X_test)):.2f}\")\n",
        "\n",
        "# Max depth 5\n",
        "reg_5 = DecisionTreeRegressor(max_depth=5)\n",
        "reg_5.fit(X_train, y_train)\n",
        "print(f\"Max Depth 5 MSE: {mean_squared_error(y_test, reg_5.predict(X_test)):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydWt9MkUBsX0",
        "outputId": "28101dd3-ab5b-43c3-f56d-090deafed079"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Tree MSE: 5179.63\n",
            "Max Depth 5 MSE: 3373.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.) Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy."
      ],
      "metadata": {
        "id": "VtUjngNiAGKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "path = DecisionTreeClassifier().cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "for alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(ccp_alpha=alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(f\"Alpha {alpha:.4f} | Test Accuracy: {clf.score(X_test, y_test):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYDOzzr-BsJf",
        "outputId": "0d237fb9-3534-4185-c6ab-cb85eb991e07"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha 0.0000 | Test Accuracy: 1.00\n",
            "Alpha 0.0081 | Test Accuracy: 1.00\n",
            "Alpha 0.0111 | Test Accuracy: 1.00\n",
            "Alpha 0.0111 | Test Accuracy: 1.00\n",
            "Alpha 0.0162 | Test Accuracy: 1.00\n",
            "Alpha 0.0241 | Test Accuracy: 0.97\n",
            "Alpha 0.2433 | Test Accuracy: 0.63\n",
            "Alpha 0.3334 | Test Accuracy: 0.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.) Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score."
      ],
      "metadata": {
        "id": "JozvEIBQAGBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "print(classification_report(y_test, clf.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upDPlX5oBr7L",
        "outputId": "f524fece-383e-43ed-9c8b-fb5c94f9a91d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.) Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn."
      ],
      "metadata": {
        "id": "Tdc1fYRtAF23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "v5lqgBjDBrnp",
        "outputId": "896941a4-a16e-4664-d07d-1be78aee26b3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAGwCAYAAAD8AYzHAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJvZJREFUeJzt3Xt8FPX1//H3CmGJECIhVxA0liIgyF2agsEIFajlot9ibaENaLFIQAKikF9FQNBVtIIIQstXAS1YtRSKaO2Pb5BbIUCCAVEMcrEgQgJFExPIErLz+8Nf9+s2Ecgwk0kmr2cf88d+djNzlsc0OZ5zZsZjGIYhAAAAE65yOgAAAFB7kUgAAADTSCQAAIBpJBIAAMA0EgkAAGAaiQQAADCNRAIAAJhGIgEAAEyr73QAdijd+ZbTIaCGadw73ekQANRQF84ft/0YZacPW7KfsOgbLNmPlahIAAAA01xZkQAAoEYJlDsdgW1IJAAAsJsRcDoC25BIAABgt4B7EwlmJAAAgGlUJAAAsJlBawMAAJhGawMAAKAiKhIAANiN1gYAADDNxfeRoLUBAABMoyIBAIDdaG0AAADTuGoDAACgIioSAADYjBtSAQAA81zc2iCRAADAbi6uSDAjAQAATKMiAQCA3Vx8QyoSCQAA7EZrAwAAoCIqEgAA2I2rNgAAgGm0NgAAACqiIgEAgN1obQAAALMMw72Xf9LaAAAAplGRAADAbi4etiSRAADAbi6ekaC1AQCA3YyANVsVbd68WYMGDVLz5s3l8Xi0Zs2a0LAMQ48//rgSEhIUHh6ufv366dNPP63SMUgkAABwqZKSEnXq1EkLFy6s9P05c+Zo/vz5Wrx4sXbs2KFGjRqpf//+Ki0tvexj0NoAAMBuDj20a+DAgRo4cGCl7xmGoXnz5umxxx7TkCFDJEmvvvqq4uLitGbNGt17772XdQwqEgAA2M2i1obf71dRUVHI5vf7TYV05MgRnTx5Uv369QuuRUZGqmfPntq+fftl74dEAgCAWsLn8ykyMjJk8/l8pvZ18uRJSVJcXFzIelxcXPC9y0FrAwAAu1l01UZGRoYmTZoUsub1ei3Zt1kkEgAA2M2i+0h4vV7LEof4+HhJUn5+vhISEoLr+fn56ty582Xvh9YGAAB1UGJiouLj45WZmRlcKyoq0o4dO5SUlHTZ+6EiAQCA3Ry6IVVxcbEOHjwYfH3kyBHl5uYqKipKrVq1Unp6umbPnq3vf//7SkxM1LRp09S8eXMNHTr0so9BIgEAgN0cSiSys7OVkpISfP3v+YrU1FQtW7ZMjz76qEpKSvTAAw/oq6++Uu/evfXee++pYcOGl30Mj2EYhuWRO6x051tOh4AapnHvdKdDAFBDXTh/3PZjlG55zZL9NLz1l5bsx0pUJAAAsJmbHyNOIgEAgN1c/NAuEgkAAOzm4seIc/knAAAwjYoEAAB2o7UBAABMo7UBAABQERUJAADsRmsDAACYRmsDAACgIioSAADYjdYGAAAwzcWJBK0NAABgGhUJAADs5uJhSxIJAADs5uLWBokEAAB2c3FFghmJWi7nkyMa/7vX1G/8M+r0y8e0IfvjkPcNw9DCVf+jvuOe1i33zdADT7+if5487UywcMyDY1J18ECWiosOadvWt9Wje2enQ4KDOB9gJRKJWu6cv0w3topXRuqgSt9f+s4Wvf5/s/TYqCH644wxCvc20INzlst/vqyaI4VThg0brOeena5Zs59Xj54DtGfvx3r3nRWKiWnmdGhwAOeDQwIBa7YaiESiluvdqY3GDfuR+nZvX+E9wzC04r1tGj34NqV0a6c2reI1+zc/1amvvtaGnP0ORAsnTJwwWv/98kotf/VN7d//qcamTdXZs+c0auS9TocGB3A+OMQIWLPVQI4mEqdPn9acOXN01113KSkpSUlJSbrrrrv07LPP6tSpU06G5grHT32p04XF6tnhe8G1iKsbquMN12rvwWMORobqEhYWpq5db1bmhi3BNcMwlLlhq37wg24ORgYncD7ADo4lErt27VKbNm00f/58RUZGKjk5WcnJyYqMjNT8+fPVtm1bZWdnX3I/fr9fRUVFIRtl+2+c/qpYktQssnHIerPIxjpd+LUTIaGaRUdHqX79+irID52LKSg4pfi4GIeiglM4Hxzk4taGY1dtjB8/XsOGDdPixYvl8XhC3jMMQ2PGjNH48eO1ffv2i+7H5/Np5syZIWu//fVP9djoeyyPGQAAU2poEmAFxyoSe/bs0cSJEyskEZLk8Xg0ceJE5ebmXnI/GRkZKiwsDNkeSb3Lhohrn+hrvqlE/KuwOGT9X4XFio6McCIkVLPTp8/owoULio2LDlmPjY3RyXzah3UN5wPs4FgiER8fr507d37n+zt37lRcXNwl9+P1etWkSZOQzdsgzMpQa60WMU0VHdlYOz46FFwrPleqDw9/rptbt3QwMlSXsrIy7d69V7en9A6ueTwe3Z7SW1lZOQ5GBidwPjjIMKzZaiDHWhuTJ0/WAw88oJycHPXt2zeYNOTn5yszM1NLlizRc88951R4tcbZUr+O5p8Jvj5+6kt98s8TimwUroToazR8wA+15K8bdV18M7WIaaqFf85UzDURur1bOwejRnWa+8ISLX15rnJ279WuXR/oofGj1ahRuJYtf8Pp0OAAzgeHuLi14VgikZaWpujoaM2dO1cvvfSSysvLJUn16tVTt27dtGzZMt1zD3MOl/LRkeP69VOvBF8/t/JvkqTBvbto1m/+S6PuvFXn/Of1xCt/1ddnS9WlTSu99EgqVZs65K231iomOkozHp+s+PgY7dnzke78yQgVFHBjsrqI8wFW8xiG87WSsrIynT79zUkcHR2tsLAr+yNXuvMtK8KCizTune50CABqqAvnj9t+jHMrplmyn/DhsyzZj5VqxLM2wsLClJCQ4HQYAADYo4beTMoKNSKRAADA1Vw8I8EtsgEAgGlUJAAAsJvz44i2IZEAAMButDYAAAAqoiIBAIDdXFyRIJEAAMBuLr78k9YGAAAwjYoEAAA2MwJctQEAAMxy8YwErQ0AAGAaFQkAAOzm4mFLEgkAAOzGjAQAADCNGQkAAICKqEgAAGA3F1ckSCQAALCbi5/+SWsDAACYRkUCAAC70doAAACmufjyT1obAADANCoSAADYjTtbAgAA02htAAAAVERFAgAAmxlctQEAAExzcWuDRAIAALu5eNiSGQkAAGAaiQQAAHYLGNZsVVBeXq5p06YpMTFR4eHh+t73vqdZs2bJsPi5H7Q2AACwmwPDls8884wWLVqk5cuX66abblJ2drZGjRqlyMhIPfTQQ5Ydh0QCAAAX2rZtm4YMGaI777xTknT99dfr9ddf186dOy09Dq0NAADsZlFrw+/3q6ioKGTz+/2VHvKHP/yhMjMzdeDAAUnSnj17tHXrVg0cONDSr0YiAQCA3YyAJZvP51NkZGTI5vP5Kj3k1KlTde+996pt27YKCwtTly5dlJ6eruHDh1v61WhtAABQS2RkZGjSpEkha16vt9LPvvnmm1qxYoVWrlypm266Sbm5uUpPT1fz5s2VmppqWUwkEgAA2M2iG1J5vd7vTBz+0yOPPBKsSkhSx44d9c9//lM+n49EAgCA2sSJW2SfPXtWV10VOsFQr149BSyOhUQCAAAXGjRokJ588km1atVKN910kz744AM9//zzuu+++yw9DokEAAB2c+BZGy+++KKmTZumsWPHqqCgQM2bN9dvfvMbPf7445Yeh0QCAAC7OZBIREREaN68eZo3b56txyGRAADAbjy0CwAAoCIqEgAA2M2B1kZ1IZEAAMBmhosTCVobAADANCoSAADYzcUVCRIJAADs5sCdLasLrQ0AAGAaFQkAAOxGawMAAJjm4kSC1gYAADCNigQAADYzDPdWJEgkAACwm4tbGyQSAADYzcWJBDMSAADANFdWJBr3Tnc6BNQwXy+9z+kQUINEjHrF6RBQx7j5WRuuTCQAAKhRXJxI0NoAAACmUZEAAMBu7n3UBokEAAB2c/OMBK0NAABgGhUJAADs5uKKBIkEAAB2c/GMBK0NAABgGhUJAABs5uZhSxIJAADs5uLWBokEAAA2c3NFghkJAABgGhUJAADsRmsDAACYZbg4kaC1AQAATKMiAQCA3VxckSCRAADAZrQ2AAAAKkFFAgAAu7m4IkEiAQCAzdzc2iCRAADAZm5OJJiRAAAAplGRAADAZm6uSJBIAABgN8PjdAS2obUBAABMoyIBAIDNaG0AAADTjACtDQAAgAqoSAAAYDNaGwAAwDSDqzYAAAAqoiIBAIDNaG0AAADT3HzVBokEAAA2MwynI7APMxIAAMA0KhIAANiM1gYAADDNzYkErQ0AAGAaFQkAAGzm5mFLEgkAAGxGawMAAKASJBIAANjMMDyWbFV1/PhxjRgxQs2aNVN4eLg6duyo7OxsS78brQ0AAGzmxC2yv/zyS/Xq1UspKSn629/+ppiYGH366adq2rSppcchkQAAoJbw+/3y+/0ha16vV16vt8Jnn3nmGbVs2VJLly4NriUmJloeE60NAABsFjA8lmw+n0+RkZEhm8/nq/SYa9euVffu3TVs2DDFxsaqS5cuWrJkieXfzWMY7rsopX6DFk6HgBrm66X3OR0CapCIUa84HQJqkAvnj9t+jLy2Ay3Zz/V71lx2RaJhw4aSpEmTJmnYsGHatWuXJkyYoMWLFys1NdWSeCRaGwAA2M6qyz+/K2moTCAQUPfu3fXUU09Jkrp06aJ9+/ZZnkjQ2gAAwIUSEhLUvn37kLV27drp6NGjlh6HigQAADZzYoigV69eysvLC1k7cOCArrvuOkuPY6oisWXLFo0YMUJJSUk6fvyb3tJrr72mrVu3WhocAABuYAQ8lmxVMXHiRGVlZempp57SwYMHtXLlSv3hD39QWlqapd+tyonEqlWr1L9/f4WHh+uDDz4IDn0UFhYG+zAAAMBZPXr00OrVq/X666+rQ4cOmjVrlubNm6fhw4dbepwqJxKzZ8/W4sWLtWTJEoWFhQXXe/Xqpd27d1saHAAAbmDV5Z9V9ZOf/EQffvihSktLtX//fo0ePdry71blGYm8vDwlJydXWI+MjNRXX31lRUwAALiKmdtb1xZVrkjEx8fr4MGDFda3bt2qG264wZKgAABA7VDlRGL06NGaMGGCduzYIY/Hoy+++EIrVqzQ5MmT9eCDD9oRIwAAtZphWLPVRFVubUydOlWBQEB9+/bV2bNnlZycLK/Xq8mTJ2v8+PF2xAgAQK1mZr6htqhyRcLj8ei3v/2tzpw5o3379ikrK0unTp3SrFmz7IgPJj04JlUHD2SpuOiQtm19Wz26d3Y6JDikxF+mOX//QANfWKeeT63Sr17J1L7jZ5wOCw7i9wOsZPrOlg0aNFD79u11yy23qHHjxlbGhCs0bNhgPffsdM2a/bx69BygPXs/1rvvrFBMTDOnQ4MDZr6drazD+Zo9tKfeGnOHkm6I05g/blJ+0VmnQ4MD+P3gDMPwWLLVRFV+aFdKSoo8nu/+Mhs2bLjioK5UXX9o17atb2tX9h5NSH9M0jdVpM8O79LCl5ZqzrMLHY7OGXX1oV2lZRfU6+nVmvuzXkpu0zy4/vMl69Xre/Ead3tHB6NzTl1+aBe/Hyqqjod27W45xJL9dD32V0v2Y6Uqz0h07tw55HVZWZlyc3O1b98+Sx8CAnPCwsLUtevNenrOguCaYRjK3LBVP/hBNwcjgxPKA4bKDUPe+vVC1r316+mDY6cdigpO4feDc9w8I1HlRGLu3LmVrs+YMUPFxcVXHNC3HTt2TNOnT9crr3z3fz34/f4Kj1Q1DOOiVRM3i46OUv369VWQH/pHoqDglNre+D2HooJTGnnDdPO1zfSHLR8rMaaJmjXy6r19x7T383+pZRQtybqG3w+wg2VP/xwxYsRF/+CbcebMGS1fvvyin/H5fIqMjAzZjMDXlsYB1GZPDu0pGdIdc9/WLU+u0sqdn2pAh5a6qm7m2oAj3DwjYdnTP7dv366GDRtW6WfWrl170fcPHz58yX1kZGRo0qRJIWtNm7WtUhxucvr0GV24cEGxcdEh67GxMTqZf8qhqOCkllGN9fLIFJ07f0HF/jLFRITr0T9vV4trqEjUNfx+cA6tjW+5++67Q14bhqETJ04oOztb06ZNq9K+hg4dKo/Ho4vNe16qReH1euX1eqv0M25WVlam3bv36vaU3lq79u+Svvn3uD2lt15atNTh6OCk8Ab1Fd6gvorOnde2QyeV3u9mp0NCNeP3A+xQ5UQiMjIy5PVVV12lG2+8UU888YTuuOOOKu0rISFBL730koYMqXyaNTc3V926MQBUVXNfWKKlL89Vzu692rXrAz00frQaNQrXsuVvOB0aHLDt4EkZMnR9swgdPVOsuf+zV4nRERrSOdHp0OAAfj84o4belNISVUokysvLNWrUKHXs2FFNmza94oN369ZNOTk535lIXKpagcq99dZaxURHacbjkxUfH6M9ez7SnT8ZoYICpvTroq/9ZXpxw17lF51TZHgD9W13rcaldFBYPctGpFCL8PvBGW5ubVT5PhINGzbU/v37lZh45f81s2XLFpWUlGjAgAGVvl9SUqLs7Gz16dOnSvut6/eRQEV19T4SqFxdvo8EKqqO+0hsS/gvS/bzwxOrLNmPlarc2ujQoYMOHz5sSSJx6623XvT9Ro0aVTmJAACgpqmpV1xYocq1zdmzZ2vy5Mlat26dTpw4oaKiopANAACECli01USXXZF44okn9PDDD+vHP/6xJGnw4MEhV0f8+yZQ5eXl1kcJAABqpMtOJGbOnKkxY8bo/ffftzMeAABcx5B7WxuXnUj8eyaTmQUAAKom4OILEKs0bFmXb/QEAIBZASoS32jTps0lk4kzZ85cUUAAAKD2qFIiMXPmzAp3tgQAABfHjMT/d++99yo2NtauWAAAcKWaeummFS77PhLMRwAAgP9U5as2AABA1dDakBQIuLkwAwCAfdz8F5TH/wEAANOq/NAuAABQNW6uSJBIAABgMzfPSNDaAAAAplGRAADAZgH3FiRIJAAAsBvP2gAAAKa5+U5MzEgAAADTqEgAAGAzLv8EAACmBVz8vCpaGwAAwDQqEgAA2MzNw5YkEgAA2MzNMxK0NgAAgGlUJAAAsBl3tgQAAKa5+c6WtDYAAIBpVCQAALAZV20AAADTmJEAAACmcfknAABAJahIAABgM2YkAACAaW6ekaC1AQAATKMiAQCAzdw8bEkiAQCAzdycSNDaAAAAplGRAADAZoaLhy1JJAAAsBmtDQAAUKs9/fTT8ng8Sk9Pt3S/VCQAALCZ0xWJXbt26fe//71uvvlmy/dNRQIAAJsZFm1+v19FRUUhm9/vv+ixi4uLNXz4cC1ZskRNmza1/LuRSAAAYLOAx5rN5/MpMjIyZPP5fBc9dlpamu68807169fPlu9GawMAgFoiIyNDkyZNClnzer3f+fk//elP2r17t3bt2mVbTCQSAADYzKoZCa/Xe9HE4duOHTumCRMmaP369WrYsKFFEVREIgEAgM2cGLbMyclRQUGBunbtGlwrLy/X5s2btWDBAvn9ftWrV++Kj0MiAQCAC/Xt21cffvhhyNqoUaPUtm1bTZkyxZIkQiKRAADAdoYDx4yIiFCHDh1C1ho1aqRmzZpVWL8SJBIAANgswC2yAQBAbbdx40bL90kiAQCAzZy+s6WdSCQAALCZEzMS1YU7WwIAANOoSAAAYLOAi2sSJBKoEyJGveJ0CKhBzn2xxekQUMcwIwEAAExzbz2CGQkAAHAFqEgAAGAzWhsAAMA0N9/ZktYGAAAwjYoEAAA24/JPAABgmnvTCFobAADgClCRAADAZly1AQAATHPzjAStDQAAYBoVCQAAbObeegSJBAAAtmNGAgAAmMaMBAAAQCWoSAAAYDP31iNIJAAAsJ2bZyRobQAAANOoSAAAYDPDxc0NEgkAAGxGawMAAKASVCQAALCZm+8jQSIBAIDN3JtG0NoAAABXgIoEAAA2o7UBAABMc/NVGyQSAADYzM33kWBGAgAAmEZFAgAAm9HaAAAAptHaAAAAqAQVCQAAbEZrAwAAmBYwaG0AAABUQEUCAACbubceQSIBAIDt3HyLbFobAADANCoSAADYzM33kSCRAADAZlz+CQAATGNGAgAAoBJUJAAAsBkzEgAAwDQ3z0jQ2gAAAKZRkQAAwGaGi5+1QSIBAIDNuGoDAACgElQkAACwmZuHLUkkAACwmZsv/6S1AQAATKMiAQCAzRi2BAAAphmGYclWFT6fTz169FBERIRiY2M1dOhQ5eXlWf7dSCQAALBZwKKtKjZt2qS0tDRlZWVp/fr1Kisr0x133KGSkhIrvlIQrQ0AAFzovffeC3m9bNkyxcbGKicnR8nJyZYdh0QCAACbWXXVht/vl9/vD1nzer3yer2X/NnCwkJJUlRUlCWx/ButDZd6cEyqDh7IUnHRIW3b+rZ6dO/sdEhwEOdD3ZWd+6HSHp2ulMHD1aHXQGVu3hby/vqN/9Do9P+jXgPvUYdeA/XJgUMORepuARmWbD6fT5GRkSGbz+e79PEDAaWnp6tXr17q0KGDpd+NRMKFhg0brOeena5Zs59Xj54DtGfvx3r3nRWKiWnmdGhwAOdD3XbuXKlubH2Dfvvw2MrfLy1V15tv0sQH76vmyGBGRkaGCgsLQ7aMjIxL/lxaWpr27dunP/3pT5bHRGvDhSZOGK3/fnmllr/6piRpbNpU/XhgX40aea/mPLvQ4ehQ3Tgf6rZbk3ro1qQe3/n+4AF9JUnHT+RXV0h1klUP7brcNsa3jRs3TuvWrdPmzZt17bXXWhLHt1GRcJmwsDB17XqzMjdsCa4ZhqHMDVv1gx90czAyOIHzAagZrGptVIVhGBo3bpxWr16tDRs2KDEx0Zbv5ngice7cOW3dulUff/xxhfdKS0v16quvXvTn/X6/ioqKQjY3P671UqKjo1S/fn0V5J8OWS8oOKX4uBiHooJTOB+AuistLU1//OMftXLlSkVEROjkyZM6efKkzp07Z+lxHE0kDhw4oHbt2ik5OVkdO3ZUnz59dOLEieD7hYWFGjVq1EX3UdngiRH42u7QAQC4bIZF/6uKRYsWqbCwULfddpsSEhKC2xtvvGHpd3M0kZgyZYo6dOiggoIC5eXlKSIiQr169dLRo0cvex+VDZ54roqwMeqa7fTpM7pw4YJi46JD1mNjY3Qy/5RDUcEpnA9AzRAwDEu2qviuu2OOHDnS0u/maCKxbds2+Xw+RUdHq3Xr1nr77bfVv39/3XrrrTp8+PBl7cPr9apJkyYhm8fjsTnymqusrEy7d+/V7Sm9g2sej0e3p/RWVlaOg5HBCZwPAOzm6FUb586dU/36/xuCx+PRokWLNG7cOPXp00crV650MLraa+4LS7T05bnK2b1Xu3Z9oIfGj1ajRuFattzachZqB86Huu3s2XM6+vkXwdfHv8jXJwcOKbJJhBLiY1VY9LVOnCxQwel/SZKOHP1ckhTdrKmim1l746K6zM2Te44mEm3btlV2drbatWsXsr5gwQJJ0uDBg50Iq9Z76621iomO0ozHJys+PkZ79nykO38yQgUFpy/9w3Adzoe6bd8nn+q+8VOCr+e8+AdJ0pCB/fTkYw/r/S1Zeuyp54PvPzL9aUnSg/cNV9r9I6o3WBdz89M/PYaDlzj4fD5t2bJF7777bqXvjx07VosXL1YgULVHldRv0MKK8AC41Lkvtlz6Q6gzwqJvsP0YSS1SLNnP9uPvW7IfKzmaSNiFRALAxZBI4NtIJK4Md7YEAMBmLvxv9iASCQAAbObmGQnH72wJAABqLyoSAADYrKp3paxNSCQAALCZm2ckaG0AAADTqEgAAGAzNw9bkkgAAGAzWhsAAACVoCIBAIDNaG0AAADTuPwTAACYFmBGAgAAoCIqEgAA2IzWBgAAMI3WBgAAQCWoSAAAYDNaGwAAwDRaGwAAAJWgIgEAgM1obQAAANNobQAAAFSCigQAADajtQEAAEwzjIDTIdiGRAIAAJu5+THizEgAAADTqEgAAGAzw8VXbZBIAABgM1obAAAAlaAiAQCAzWhtAAAA07izJQAAQCWoSAAAYDPubAkAAExz84wErQ0AAGAaFQkAAGzm5vtIkEgAAGAzN7c2SCQAALAZl38CAABUgooEAAA2o7UBAABMc/OwJa0NAABgGhUJAABsRmsDAACYxlUbAAAAlaAiAQCAzXhoFwAAMI3WBgAAQCWoSAAAYDOu2gAAAKYxIwEAAExzc0WCGQkAAFxs4cKFuv7669WwYUP17NlTO3futHT/JBIAANjMMAxLtqp64403NGnSJE2fPl27d+9Wp06d1L9/fxUUFFj23TyGC+st9Ru0cDoEADXYuS+2OB0CapCw6BtsP4ZVf5cunD9epc/37NlTPXr00IIFCyRJgUBALVu21Pjx4zV16lRLYqIiAQBALeH3+1VUVBSy+f3+Sj97/vx55eTkqF+/fsG1q666Sv369dP27dsti8mVw5ZVzdjcyO/3y+fzKSMjQ16v1+lwUANwTuDbOB+ql1V/l2bMmKGZM2eGrE2fPl0zZsyo8NnTp0+rvLxccXFxIetxcXH65JNPLIlHcmlrA1JRUZEiIyNVWFioJk2aOB0OagDOCXwb50Pt5Pf7K1QgvF5vpcngF198oRYtWmjbtm1KSkoKrj/66KPatGmTduzYYUlMrqxIAADgRt+VNFQmOjpa9erVU35+fsh6fn6+4uPjLYuJGQkAAFyoQYMG6tatmzIzM4NrgUBAmZmZIRWKK0VFAgAAl5o0aZJSU1PVvXt33XLLLZo3b55KSko0atQoy45BIuFSXq9X06dPZ4gKQZwT+DbOh7rhZz/7mU6dOqXHH39cJ0+eVOfOnfXee+9VGMC8EgxbAgAA05iRAAAAppFIAAAA00gkAACAaSQSAADANBIJl7L7sbGoPTZv3qxBgwapefPm8ng8WrNmjdMhwUE+n089evRQRESEYmNjNXToUOXl5TkdFmoxEgkXqo7HxqL2KCkpUadOnbRw4UKnQ0ENsGnTJqWlpSkrK0vr169XWVmZ7rjjDpWUlDgdGmopLv90oep4bCxqJ4/Ho9WrV2vo0KFOh4Ia4tSpU4qNjdWmTZuUnJzsdDiohahIuEx1PTYWgDsUFhZKkqKiohyOBLUViYTLXOyxsSdPnnQoKgA1USAQUHp6unr16qUOHTo4HQ5qKW6RDQB1VFpamvbt26etW7c6HQpqMRIJl6mux8YCqN3GjRundevWafPmzbr22mudDge1GK0Nl6mux8YCqJ0Mw9C4ceO0evVqbdiwQYmJiU6HhFqOioQLVcdjY1F7FBcX6+DBg8HXR44cUW5urqKiotSqVSsHI4MT0tLStHLlSv31r39VREREcHYqMjJS4eHhDkeH2ojLP11qwYIFevbZZ4OPjZ0/f7569uzpdFhwwMaNG5WSklJhPTU1VcuWLav+gOAoj8dT6frSpUs1cuTI6g0GrkAiAQAATGNGAgAAmEYiAQAATCORAAAAppFIAAAA00gkAACAaSQSAADANBIJAABgGokEAAAwjUQCcKGRI0dq6NChwde33Xab0tPTqz2OjRs3yuPx6Kuvvqr2YwOoHiQSQDUaOXKkPB6PPB6PGjRooNatW+uJJ57QhQsXbD3uX/7yF82aNeuyPssffwBVwUO7gGo2YMAALV26VH6/X++++67S0tIUFhamjIyMkM+dP39eDRo0sOSYUVFRluwHAP4TFQmgmnm9XsXHx+u6667Tgw8+qH79+mnt2rXBdsSTTz6p5s2b68Ybb5QkHTt2TPfcc4+uueYaRUVFaciQIfrss8+C+ysvL9ekSZN0zTXXqFmzZnr00Uf1n4/Q+c/Wht/v15QpU9SyZUt5vV61bt1aL7/8sj777LPgA76aNm0qj8cTfJBTIBCQz+dTYmKiwsPD1alTJ/35z38OOc67776rNm3aKDw8XCkpKSFxAnAnEgnAYeHh4Tp//rwkKTMzU3l5eVq/fr3WrVunsrIy9e/fXxEREdqyZYv+8Y9/qHHjxhowYEDwZ373u99p2bJleuWVV7R161adOXNGq1evvugxf/WrX+n111/X/PnztX//fv3+979X48aN1bJlS61atUqSlJeXpxMnTuiFF16QJPl8Pr366qtavHixPvroI02cOFEjRozQpk2bJH2T8Nx9990aNGiQcnNz9etf/1pTp061658NQE1hAKg2qampxpAhQwzDMIxAIGCsX7/e8Hq9xuTJk43U1FQjLi7O8Pv9wc+/9tprxo033mgEAoHgmt/vN8LDw42///3vhmEYRkJCgjFnzpzg+2VlZca1114bPI5hGEafPn2MCRMmGIZhGHl5eYYkY/369ZXG+P777xuSjC+//DK4Vlpaalx99dXGtm3bQj57//33Gz//+c8NwzCMjIwMo3379iHvT5kypcK+ALgLMxJANVu3bp0aN26ssrIyBQIB/eIXv9CMGTOUlpamjh07hsxF7NmzRwcPHlRERETIPkpLS3Xo0CEVFhbqxIkT6tmzZ/C9+vXrq3v37hXaG/+Wm5urevXqqU+fPpcd88GDB3X27Fn96Ec/Clk/f/68unTpIknav39/SBySlJSUdNnHAFA7kUgA1SwlJUWLFi1SgwYN1Lx5c9Wv/7//N2zUqFHIZ4uLi9WtWzetWLGiwn5iYmJMHT88PLzKP1NcXCxJeuedd9SiRYuQ97xer6k4ALgDiQRQzRo1aqTWrVtf1me7du2qN954Q7GxsWrSpEmln0lISNCOHTuUnJwsSbpw4YJycnLUtWvXSj/fsWNHBQIBbdq0Sf369avw/r8rIuXl5cG19u3by+v16ujRo99ZyWjXrp3Wrl0bspaVlXXpLwmgVmPYEqjBhg8frujoaA0ZMkRbtmzRkSNHtHHjRj300EP6/PPPJUkTJkzQ008/rTVr1uiTTz7R2LFjL3oPiOuvv16pqam67777tGbNmuA+33zzTUnSddddJ4/Ho3Xr1unUqVMqLi5WRESEJk+erIkTJ2r58uU6dOiQdu/erRdffFHLly+XJI0ZM0affvqpHnnkEeXl5WnlypVatmyZ3f9EABxGIgHUYFdffbU2b96sVq1a6e6771a7du10//33q7S0NFihePjhh/XLX/5SqampSkpKUkREhO66666L7nfRokX66U9/qrFjx6pt27YaPXq0SkpKJEktWrTQzJkzNXXqVMXFxWncuHGSpFmzZmnatGny+Xxq166dBgwYoHfeeUeJiYmSpFatWmnVqlVas2aNOnXqpMWLF+upp56y8V8HQE3gMb5rIgsAAOASqEgAAADTSCQAAIBpJBIAAMA0EgkAAGAaiQQAADCNRAIAAJhGIgEAAEwjkQAAAKaRSAAAANNIJAAAgGkkEgAAwLT/Bxffu2YOslXUAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.) Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split."
      ],
      "metadata": {
        "id": "RRjMqnbmAFEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Test Accuracy:\", grid.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFclGL3WBrVZ",
        "outputId": "0f424184-b88b-40e9-ec5d-e98df27003f9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'max_depth': None, 'min_samples_split': 2}\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}